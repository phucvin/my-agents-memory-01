Summary: Summarized sections of "Common Misconceptions about Compilers" that were not discussed in the previous session.

# User Prompt
what else in https://sbaziotis.com/compilers/common-misconceptions-about-compilers.html#if-you-have-a-compiler,-you-dont-need-an-interpreter are not discussed in the previous session. summarize them for me

# Agent Response
The previous session covered **Data Locality**, **Inlining** (specifically optimization benefits), **Branch Prediction**, and **Separate Compilation**.

Here is a summary of the **other misconceptions** from the article that were not discussed:

1.  **Optimization gives us optimal programs**
    Compilers do not find "optimal" programs; they simply "improve" them. "Optimal" is hard to define (especially for running time) and difficult to achieve due to inaccurate hardware models and lack of optimal substructure in execution time.

2.  **-O3 produces much faster code than -O2**
    There is often no statistical difference between `-O2` and `-O3` (e.g., in Clang). `-O3` can sometimes degrade performance by increasing code size, which hurts instruction cache usage.

3.  **Javascript interpreters JIT at runtime because they don't know which paths are hot ahead of time**
    The main reason JITs compile at runtime is to discover **types**, which are unknown in JavaScript until execution, not just to identify hot paths.

4.  **If you have a compiler, you don't need an interpreter**
    Interpreters remain useful (e.g., easier development). While WebAssembly JITs appeared first to prove performance viability, interpreters are making a comeback (e.g., for faster startup or memory savings), as highlighted by Ben Titzer's work.

5.  **The middle-end is target/platform-independent**
    It is generally not. IRs like LLVM IR often leak target-specific details (e.g., `size_t` width, ABI details), making them dependent on the platform.

6.  **-O0 gives you fast compilation**
    `-O0` is optimized for **debuggability**, not speed. For truly fast compilation, one needs to bypass the standard pipeline (e.g., generating machine code directly) or use simpler compilers like TinyCC.

7.  **Templates are slow to compile**
    The slowness often comes from C++'s compilation model (header inclusion), not templates themselves. Languages like D (Dlang) compile templates very quickly.

8.  **Why does link-time optimization (LTO) happen at link-time?**
    Ideally, whole-program optimization should happen in the middle-end. It happens at link-time mainly because C/C++ build systems make it difficult for the compiler to see all source files at once earlier in the pipeline.

9.  **The role of the inline keyword**
    Semantically, `inline` is about linking (allowing multiple definitions). For optimization, it is merely a weak hint (`inlinehint` in LLVM), which the compiler might ignore. To force inlining, `always_inline` is required.

10. **The best production compiler to study is LLVM**
    LLVM is complex and handles many edge cases ("a monstrosity"). For learning, simpler production compilers like the Go compiler, TinyCC, or LDC/DMD are often better starting points.

11. **Undefined behavior only enables optimizations**
    Undefined behavior (UB) can also **disable** optimizations.

12. **The compiler can "simply" define undefined behavior**
    Defining UB (e.g., defining dereferencing NULL as 0) imposes performance costs (e.g., runtime checks) or requires matching hardware behavior exactly, which is complex and restrictive.

13. **99% correctness rate is ok**
    For cost models (ML), 99% accuracy is acceptable. For code generation (e.g., LLMs), it is not. A 99% correctness rate implies a massive number of bugs (e.g., 50 bugs in 5,000 lines), making the code unusable and a nightmare to debug.
