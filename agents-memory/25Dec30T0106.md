Summary: User requested a minimal LLM implementation in pure C++ with no external libraries. Created a directory `codes/minimal_llm/` and implemented a GPT-style model including `Tensor` helpers, `MultiHeadAttention`, `LayerNorm`, `FeedForward`, and a generation loop.

# User Prompt
how does a minimal LLM works

# Agent Response
I created a minimal, pure C++ implementation of a Transformer-based Language Model to demonstrate how an LLM works.

The implementation is located in `codes/minimal_llm/` and includes:
- `main.cpp`: The core implementation (Tensor logic, Layers, GPT architecture, inference loop).
- `Makefile`: For compiling the code.
- `README.md`: Explaining the code structure and how to run it.

You can run the demo by executing:
```bash
cd codes/minimal_llm
make
./main
```

The output will be random text since the weights are initialized randomly, but it demonstrates the data flow and architectural components of a modern LLM (like GPT).

# User Prompt
use C++ with no libraries
also follow agents.md

# Agent Response
I updated the plan to strictly use pure C++ with standard libraries only (no external dependencies like Torch or BLAS) and adhered to the `AGENTS.md` guidelines regarding code organization and memory recording.

I have implemented the C++ version in `codes/minimal_llm/main.cpp`.
